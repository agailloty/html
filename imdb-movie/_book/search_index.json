[
["presentation-du-projet.html", "Déterminer les facteurs explicatifs du revenu d’un film 1 Présentation du projet 1.1 Résultats globaux", " Déterminer les facteurs explicatifs du revenu d’un film Axel-Cleris Gailloty Najwa Maaden 2019-09-07 1 Présentation du projet L’Internet Movie Database (littéralement « Base de données cinématographiques d’Internet »), abrégé en IMDb, est une base de données en ligne sur le cinéma mondial, et sur la télévision. IMDb collecte des informations comme le budget consacré à la réalisation du film, le nombre de votes, la note moyenne et encore d’autres informations dans une base de donnée accessible gratuitement. L’accessibilité des données collectées par IMDb et surtout les caractéristiques qui rendent un film unique nous ont poussé à nous interroger sur les élements qui expliquent le succès d’un film. Autrement dit, dans ce projet nous nous intéressons à la question de savoir quels sont les déterminants qui entrent dans le succès mondial d’un film. 1.1 Résultats globaux A l’issue de ce projet nous avons découvert que …. L’Internet Movie Database (littéralement « Base de données cinématographiques d’Internet »), abrégé en IMDb, est une base de données en ligne sur le cinéma mondial, et sur la télévision. "],
["a-propos-des-auteurs.html", "2 A propos des auteurs", " 2 A propos des auteurs Axel-Cleris Gailloty est étudiant en Master Economie Appliquée parcours Ingénierie et Evaluation économique à l’Université d’Angers. Il est intéressé par l’analyse des données multidimensionnelles, l’économétrie et le Machine Learning. "],
["importation-des-packages.html", "3 Importation des packages", " 3 Importation des packages Pour ce travail nous travaillerons avec le langage de programmation R. Le langage R est extrêmement puissant pour l’analyse des données et la modélisation statique. Il a été pensé pour “programmer avec des donnée”, aujourd’hui ce langage est utilisé par une grande partie des professionnels en Data science. Un des grands avanatages du langage R est sa grande communauté qui chaque jour publie de nouveaux packages (code réutilisable) s’adréssant à toute sorte de besoin dans l’analyse des données. Nous présentons ici les packages que nous utiliserons dans ce travail. library(tidyverse) library(recipes) library(caret) library(FactoMineR) library(factoextra) # Reading the dataset df &lt;- read_csv(&quot;../data/tmdb-movies.csv&quot;) "],
["analyse-exploratoire-des-donnees.html", "4 Analyse exploratoire des données 4.1 Qualité des données 4.2 Traitement des données manquantes 4.3 Imputation des valeurs manquantes 4.4 L’Analyse en composantes principales", " 4 Analyse exploratoire des données Dans cette partie nous cherchons à comprendre le jeu de données. Il sera également question de nettoyer le jeu de données afin de le préparer à l’analyse. 4.1 Qualité des données Nous voulons avant toute chose nous assurer que le jeu de données dim(df) ## [1] 10866 21 # Aperçu de quelques colonnes set.seed(1221) # Aléatoire df_subset &lt;- sample(df, 6) knitr::kable( head(df_subset) ) original_title imdb_id revenue_adj genres director release_date Jurassic World tt0369610 1392445893 Action|Adventure|Science Fiction|Thriller Colin Trevorrow 6/9/15 Mad Max: Fury Road tt1392190 348161292 Action|Adventure|Science Fiction|Thriller George Miller 5/13/15 Insurgent tt2908446 271619025 Adventure|Science Fiction|Thriller Robert Schwentke 3/18/15 Star Wars: The Force Awakens tt2488496 1902723130 Action|Adventure|Science Fiction|Fantasy J.J. Abrams 12/15/15 Furious 7 tt2820852 1385748801 Action|Crime|Thriller James Wan 4/1/15 The Revenant tt1663202 490314247 Western|Drama|Adventure|Thriller Alejandro GonzÃ¡lez IÃ±Ã¡rritu 12/25/15 4.2 Traitement des données manquantes # Totale de données manquantes sum(is.na(df)) ## [1] 13434 Il y a un total de 13234 point manquants dans le jeu de données. Il se peut bien que la répartition des données manquantes diffèrent selon les colonnes. Nous allons vérifier cela. visdat::vis_miss(df, cluster = TRUE) L’image ci-dessus montre le nombre d’observations manquantes par colonnes dans le jeu de données. Sur l’ensemble du jeu de données, près de 6% des données sont manquantes. La colonne qui enregistre le plus de données manquantes est la colonnes homepage (avec 72% des données manquantes). Il est aussi possible de compter le nombre des données manquantes par colonnes pour calculer individuellement leur pourcentage. N_ROWS = nrow(df) sapply(df, function(x) sum(is.na(x))) %&gt;% stack() %&gt;% rename(n_missing = values, col = ind) %&gt;% filter(n_missing &gt; 0) %&gt;% ggplot(aes(x = col, y = n_missing)) + geom_col() + coord_flip() Avant de décider la stratégie à adopter pour les données manquantes, présentons chacune de ces colonnes. La colonne production_companies : le studio de réalisation du film genres : Il s’agit de genre du film. Exemple drame, action … overview : Il s’agit d’un bref résumé du film keywords : ce sont les mots-clés associés au film tagline : ce sont phrases chocs pour la publicité du film director : le producteur du film homepage : c’est le lien vers la page d’accueil du film cast : Ce sont les listes des principaux acteurs du film imdb_id : C’est l’identifiant du film selon IMDB Complétons cette liste de colonnes par les autres colonnes présentes dans le jeu de données : id : identifiant du film popularity : une mesure de la popularité du film attribuée par IMDB budget : le budget consacré à la réalisation du film, en dollars revenue : le revenu qu’a suscité le film. original_title : Titre original du film. runtime : la durée du film (en minute) release_date : la date de parution du film vote_count : le total des votes attribués au film vote_average : la moyenne des votes (sur 10) realease_year : année de sortie du film Revenons donc au traitement des valeurs manquantes. Présentons les differentes stratégies pour traiter les données manquantes avec leurs avantages et incovénients : Supprimer les enregistrements qui contiennent les valeurs manquantes : cette stratégie est facile à mettre en place mais l’inconvénient est que faisant ainsi nous pouvons perdre beaucoup d’informations concernant le jeu de données. Voyons le nombre d’informations potentielles que nous pouvons perdre si adoptions cette stratégie. N_ROWS - df[complete.cases(df), ] %&gt;% nrow() ## [1] 8874 Nous perdrons plus de 8874 observations dans le jeu de données. Une stratégie alternative serait par exemple de supprimer uniquement les colonnes qui contiennent le plus de valeurs manquantes et conserver celles qui ont en que peu. En général la stratégie de supprimer les observations manquantes ne marche pas bien car il est possible de perdre beaucoup d’informations. Nous allons plutôt nous focaliser sur les stratégies d’imputations des valeurs manquantes. Imputer des valeurs manquantes consiste à utiliser une stratégie qui dévine au mieux la valeur manquante dans la colonne. Pour une colonne numérique et continue, par exemple nous pouvons remplacer chaque valeur manquante par la indicateurs de distributions telles que la moyenne, la médiane ou encore le mode. Pour une colonne catégorique, nous pouvons par exemple remplacer la valeur maquante par la valeur ayant la plus grande fréquence (le mode de la distribution). Toutes ces stratégies n’ont en réalité qu’un seul but : réduire la perte d’information et préserver au mieux la vraissemblance de la distribution. Toutefois il existe des solutions plus sophistiquées plus imputer les valeurs manquantes. Ces solutions utilisent pour la plupart des algorithmes de Machine Learning et sont plus performantes; l’avantage des méthodes d’imputations de Machine Learning est qu’elles simulent la distribution des valeurs dans un espace à N dimensions alors que les méthodes d’imputation classique n’utilisent qu’une seule dimension qui est celle de la colonne. Là où une imputation par la moyenne remplace la valeur manquante par la moyenne de la colonne, une imputation utilisant le Machine Learning (par exemple K-Nearest Neighbors) va remplacer la valeur manquante par la moyenne de K individus dont la distance euclidienne dans N dimensions est la plus faible. Cela fait donc que la vraissemblance de la valeur prédite est plus haute. 4.3 Imputation des valeurs manquantes Nous allons maintenant imputer les valeurs manquantes. Toutefois à l’issue de la description que nous avions faite des colonnes, il existe des colonnes qui n’ont réellement pas de valeurs dans la prédiction des données et certaines colonnes dont imputer des valeurs n’aurait pas de sens. Les colonnes id et id_imdb ne sont que des identifiants utilisés pour distinguer les films, nous pouvons les supprimer sans risque de perdre des informations. Imputer la colonne homepage par le mode ne fait aucun sens car chaque film est unique et a forcément sa page d’accueil. Il serait plus intelligent de remplacer les valeurs manquantes par “Inconnu” car après tout cette information nous est inconnue. Nous verrons plus tard comment transformer cette colonne en un élement plus intéressant. Similairement les colonnes production_companies, director, cast, taglines, overview sont uniques à chaque film, la chance de se tromper si on utilise de l’inférence pour remplacer les valeurs manquantes qu’elles contiennent est très grande. Donc il serait nécessaire de les remplacer aussi par “Inconnu”. Nous allons faire une copie du jeu de données. Cette sur cette copie que nous appliqueront les imputations. txt_columns &lt;- select_if(df, is.character) %&gt;% colnames() num_txt_columns &lt;- length(txt_columns) unique_occurences &lt;- vector(mode = &quot;numeric&quot;, length = num_txt_columns) for (txt_col in txt_columns){ unique_occurences[txt_col] &lt;- separate_rows(df, col = txt_col, sep = &quot;[|]&quot;) %&gt;% pull(!!txt_col) %&gt;% unique() %&gt;% length() } df_uniq_col_occurences &lt;- unique_occurences[unique_occurences &gt; 0] %&gt;% data.frame() %&gt;% tibble::rownames_to_column() colnames(df_uniq_col_occurences) &lt;- c(&quot;column&quot;,&quot;uniq_occurence&quot;) ggplot(df_uniq_col_occurences, aes(x = column, y = uniq_occurence)) + geom_col() + coord_flip() Nous observons à partir de ce graphique que lorsque nous élargissons les niveaux des colonnes textuels le nombre des dudits niveaux sont trop nombreux à l’exception de la colonne genres qui en a très peu. Si nous décidons d’étendre ces niveux pour devenir des colonnes alors le nombre de colonnes dépassera forcément le nombre d’observations et le temps de calcul risque d’être très grand. Nous allons tout simplement les effacer du jeu de données. Nous pouvons également exploiter la colonne release_date pour n’extraire que les mois de l’année car nous savons qu’une année n’a que 12 niveaux. ready_df &lt;- df %&gt;% select(-c(id, tagline, production_companies, overview, original_title, keywords, imdb_id, homepage, director, cast)) # Transformons les niveaux de la colonne genre en plusieurs colonnes binaires ready_df &lt;- separate_rows(ready_df, col = &quot;genres&quot; , sep = &quot;[|]&quot;) %&gt;% distinct() %&gt;% mutate(dummy = 1) %&gt;% spread(key = &quot;genres&quot;, value = dummy, fill = 0) # Transformons les mois de sortie des films en colonnes binaires ready_df &lt;- ready_df %&gt;% mutate(release_date = as.Date.character(ready_df$release_date, format = &quot;%m/%d/%y&quot;), month = months(release_date)) %&gt;% mutate(dummy = 1) %&gt;% spread(key = &quot;month&quot;, value = dummy, fill = 0) # Supprimer la colonne NA ready_df &lt;- select(ready_df, -c(&quot;&lt;NA&gt;&quot;, &quot;release_date&quot;)) df_subset &lt;- sample(ready_df, 6) knitr::kable( head(df_subset) ) janvier mars novembre Comedy Mystery TV Movie 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 Les deux transformations ue nous venons d’appliquer sur le jeu de données initial (étendre les niveaux de la colonne genres et ceux de la colonne release_date transformée en data) font que dès maintenant le jeu de données contient 43 colonnes en total. L’idée pour le reste du travail à venir est de réduire les dimensions du jeu de données par une opération de diagonalisation du genre l’Analyse en Composante Principale (ACP) en vue d’approfondir notre connaissance du jeu de données. Le bloc de code suivant vérifie que le jeu de données ne contient plus aucune valeur manquante et le prépare à être utilisé dans une ACP. sum(is.na(ready_df)) ## [1] 0 Les transformations que nous avions appliquées sur le jeu de données ont permis d’enlever les valeurs manquantes. Rappellons que les valeurs manquantes se concentraient principalement dans les colonnes non-numériques avec le plus grand ratio étant celui de la colonne homepage. 4.4 L’Analyse en composantes principales acp &lt;- PCA(ready_df, graph = FALSE, scale.unit = TRUE) Les résultats de l’ACP sont enregistrés dans la variable acp créée pour cet effet. Nous voulons maintenant observer et interpréter les résultats afin d’avoir une large compréhension du jeu de données. Il est possible d’imprimer les résultats de l’ACP sur l’écran grâce à la fonction summary() mais procéder ainsi ne nous sera pas d’une grande utilité. Il est plus intéressant de visualiser ces résultats. C’est ce que nous ferons dans la partie visualisation. "],
["visualisation-des-donnees.html", "5 Visualisation des données 5.1 Visualisation des composantes principales 5.2 Nuage de points 5.3 Correlations 5.4 Densité des colonnes et normalisations 5.5 Colonnes catégoriques", " 5 Visualisation des données fviz_screeplot(acp) fviz_pca_var(acp, repel = TRUE) 5.1 Visualisation des composantes principales 5.2 Nuage de points ready_df %&gt;% ggplot(aes(x = budget, y = revenue)) + geom_point() + geom_smooth(method = &quot;lm&quot;) + labs(title = &quot; Relation entre le budget dépensé et le revenu&quot;) Lien entre la popularité et le budget ready_df %&gt;% ggplot(aes(x = budget, y = popularity)) + geom_point() + geom_smooth(method = &quot;lm&quot;) + labs(title = &quot; Relation entre le budget dépensé et la popularité du film&quot;) Lien entre la popularité et le revenu du film ready_df %&gt;% ggplot(aes(x = revenue, y = popularity)) + geom_point() + geom_smooth(method = &quot;lm&quot;) + labs(title = &quot; Relation entre le revenu et la popularité du film&quot;) 5.3 Correlations ggcorrplot::ggcorrplot( cor(ready_df), colors = c(&quot;green&quot;, &quot;white&quot;, &quot;red&quot;, hc.order = TRUE, type = &quot;lower&quot;) ) 5.4 Densité des colonnes et normalisations ready_df %&gt;% select(revenue, budget, popularity, vote_average) %&gt;% mutate(id = &quot;corr&quot;) %&gt;% reshape2::melt(id.var = &quot;id&quot;) %&gt;% ggplot(aes(x = value)) + geom_histogram( bins = 100) + facet_wrap(~variable, ncol = 4, scale = &quot;free&quot;) A l’exception de la colonne vote_average, toutes les autres colonnes sont loin de suivre une distribution normale. Il y a évidemment dans ces colonnes des valeurs atypiques (outliers). Représentons à l’aide des boîtes à moustaches ces colonnes ready_df %&gt;% select(revenue, budget, popularity, vote_average) %&gt;% mutate(id = &quot;corr&quot;) %&gt;% reshape2::melt(id.var = &quot;id&quot;) %&gt;% ggplot(aes(y = value, x = &quot;Distribution&quot;)) + geom_boxplot() + facet_wrap(~variable, ncol = 2, scale = &quot;free&quot;) L’intuition que nous avions eu sur les valeurs atypiques s’avèrent à la vue de ces colonnes. Mettons dans un tableau l’ensemble des indicateurs de moments pour chacune de ces colonnes afin d’observer là il y a anomalie dans la distribution des données. print(&quot;Indicateurs de tendance centrale du revenu&quot;) ## [1] &quot;Indicateurs de tendance centrale du revenu&quot; quantile(ready_df$revenue) ## 0% 25% 50% 75% 100% ## 0 0 0 24000000 2781505847 print(&quot;Indicateurs de tendance centrale du budget&quot;) ## [1] &quot;Indicateurs de tendance centrale du budget&quot; quantile(ready_df$budget) ## 0% 25% 50% 75% 100% ## 0.00e+00 0.00e+00 0.00e+00 1.50e+07 4.25e+08 Il semble bien qu’il y ait un problème avec les données car le minimum des revenus est 0, cela ne peut se faire ready_df %&gt;% select(revenue, budget, popularity, vote_average) %&gt;% mutate(id = &quot;corr&quot;) %&gt;% reshape2::melt(id.var = &quot;id&quot;) %&gt;% ggplot(aes(x = value)) + geom_density() + facet_wrap(~variable, ncol = 4, scale = &quot;free&quot;) Il serait intéressant de normaliser chacune de ces colonnes pour les rapprocher d’une distribution normale. Si nous essayons d’estimer le revenu du film à partir de ces colonnes numériques avec une régression linéaire OLS, nous risquons de construire un modèle biaisé avec cette configuration. Il existe une transformation qui porte le nom de ces auteurs Yeo et Johnson qui consiste à rapprocher vers une distribution normale des colonnes qui ne le sont pas. Nous allons essayer cette trasnformation pour voir si nous pouvons améliorer la qualité d’un modèle. (blueprint &lt;- recipe(revenue~., data = ready_df) %&gt;% step_YeoJohnson(all_predictors(), budget, vote_average, popularity, revenue)) ## Data Recipe ## ## Inputs: ## ## role #variables ## outcome 1 ## predictor 40 ## ## Operations: ## ## Yeo-Johnson transformation on all_predictors(), ... blueprint &lt;- prep(blueprint) new_df &lt;- blueprint$template new_df %&gt;% select(revenue, budget, popularity, vote_average) %&gt;% mutate(id = &quot;corr&quot;) %&gt;% reshape2::melt(id.var = &quot;id&quot;) %&gt;% ggplot(aes(x = value)) + geom_density() + facet_wrap(~variable, ncol = 4, scale = &quot;free&quot;) 5.5 Colonnes catégoriques ready_df %&gt;% select(Action:septembre) %&gt;% mutate_all(~as.factor(.)) %&gt;% mutate(id = &quot;categorical&quot;) %&gt;% reshape2::melt(id.var = &quot;id&quot;) %&gt;% ggplot(aes(x = value)) +geom_bar() + facet_wrap(~variable, ncol = 4, scale = &quot;free&quot;) "],
["modelisation.html", "6 Modélisation 6.1 Regression linéaire simple 6.2 Régression linéaire multiple 6.3 Régularisation", " 6 Modélisation 6.1 Regression linéaire simple 6.2 Régression linéaire multiple 6.3 Régularisation 6.3.1 Régression contrainte Ridge 6.3.2 Régression contrainte LASSO "],
["evaluation-des-modeles.html", "7 Evaluation des modèles", " 7 Evaluation des modèles "],
["conclusion.html", "8 Conclusion", " 8 Conclusion Ce travail nous a permis de nous rendre de certaines difficultés rencontrées dans les projets de data science à savoir la préparation, le nettoyage des données. Si nous avons pu trouver que le budget initial consacré à la réalisation du film joue un rôle essentiel dans la …… Toutefois les résultats que nous avons trouvés doivent être pris avec un regard critique. "],
["note-sur-la-reproductibilite.html", "9 Note sur la reproductibilité", " 9 Note sur la reproductibilité Ces travaux ont été menés avec les environnements suivants : sessionInfo() ## R version 3.5.1 (2018-07-02) ## Platform: x86_64-w64-mingw32/x64 (64-bit) ## Running under: Windows 10 x64 (build 18362) ## ## Matrix products: default ## ## locale: ## [1] LC_COLLATE=French_France.1252 LC_CTYPE=French_France.1252 ## [3] LC_MONETARY=French_France.1252 LC_NUMERIC=C ## [5] LC_TIME=French_France.1252 ## ## attached base packages: ## [1] stats graphics grDevices utils datasets methods base ## ## other attached packages: ## [1] factoextra_1.0.5 FactoMineR_1.41 caret_6.0-84 lattice_0.20-35 ## [5] recipes_0.1.5 forcats_0.3.0 stringr_1.4.0 dplyr_0.8.1 ## [9] purrr_0.3.2 readr_1.3.1 tidyr_0.8.2 tibble_2.1.3 ## [13] ggplot2_3.2.0 tidyverse_1.2.1 ## ## loaded via a namespace (and not attached): ## [1] httr_1.4.0 jsonlite_1.6 splines_3.5.1 ## [4] foreach_1.4.4 prodlim_2018.04.18 modelr_0.1.3 ## [7] assertthat_0.2.1 highr_0.7 stats4_3.5.1 ## [10] cellranger_1.1.0 ggrepel_0.8.0 yaml_2.2.0 ## [13] ipred_0.9-8 pillar_1.3.1 backports_1.1.4 ## [16] glue_1.3.1 visdat_0.5.3 digest_0.6.18 ## [19] rvest_0.3.2 colorspace_1.4-0 htmltools_0.3.6 ## [22] Matrix_1.2-14 plyr_1.8.4 timeDate_3043.102 ## [25] pkgconfig_2.0.2 ggcorrplot_0.1.2 broom_0.5.2 ## [28] haven_2.1.0 bookdown_0.13 scales_1.0.0 ## [31] gower_0.2.0 lava_1.6.5 generics_0.0.2 ## [34] ggpubr_0.2 withr_2.1.2 nnet_7.3-12 ## [37] lazyeval_0.2.1 cli_1.1.0 survival_2.42-3 ## [40] magrittr_1.5 crayon_1.3.4 readxl_1.2.0 ## [43] evaluate_0.14 nlme_3.1-137 MASS_7.3-51.1 ## [46] xml2_1.2.0 class_7.3-15 rsconnect_0.8.13 ## [49] tools_3.5.1 data.table_1.12.2 hms_0.4.2 ## [52] munsell_0.5.0 cluster_2.0.7-1 packrat_0.5.0 ## [55] flashClust_1.01-2 compiler_3.5.1 rlang_0.3.4 ## [58] grid_3.5.1 iterators_1.0.10 rstudioapi_0.9.0 ## [61] leaps_3.0 labeling_0.3 rmarkdown_1.13 ## [64] gtable_0.2.0 ModelMetrics_1.2.2 codetools_0.2-15 ## [67] reshape2_1.4.3 R6_2.4.0 lubridate_1.7.4 ## [70] knitr_1.23 stringi_1.2.4 Rcpp_1.0.1 ## [73] rpart_4.1-13 scatterplot3d_0.3-41 tidyselect_0.2.5 ## [76] xfun_0.7 "],
["bibliographie.html", "10 Bibliographie", " 10 Bibliographie citation(package = &quot;tidyverse&quot;) ## ## To cite package &#39;tidyverse&#39; in publications use: ## ## Hadley Wickham (2017). tidyverse: Easily Install and Load the ## &#39;Tidyverse&#39;. R package version 1.2.1. ## https://CRAN.R-project.org/package=tidyverse ## ## A BibTeX entry for LaTeX users is ## ## @Manual{, ## title = {tidyverse: Easily Install and Load the &#39;Tidyverse&#39;}, ## author = {Hadley Wickham}, ## year = {2017}, ## note = {R package version 1.2.1}, ## url = {https://CRAN.R-project.org/package=tidyverse}, ## } citation(package = &quot;caret&quot;) ## ## To cite package &#39;caret&#39; in publications use: ## ## Max Kuhn. Contributions from Jed Wing, Steve Weston, Andre ## Williams, Chris Keefer, Allan Engelhardt, Tony Cooper, Zachary ## Mayer, Brenton Kenkel, the R Core Team, Michael Benesty, Reynald ## Lescarbeau, Andrew Ziem, Luca Scrucca, Yuan Tang, Can Candan and ## Tyler Hunt. (2019). caret: Classification and Regression ## Training. R package version 6.0-84. ## https://CRAN.R-project.org/package=caret ## ## A BibTeX entry for LaTeX users is ## ## @Manual{, ## title = {caret: Classification and Regression Training}, ## author = {Max Kuhn. Contributions from Jed Wing and Steve Weston and Andre Williams and Chris Keefer and Allan Engelhardt and Tony Cooper and Zachary Mayer and Brenton Kenkel and the R Core Team and Michael Benesty and Reynald Lescarbeau and Andrew Ziem and Luca Scrucca and Yuan Tang and Can Candan and Tyler Hunt.}, ## year = {2019}, ## note = {R package version 6.0-84}, ## url = {https://CRAN.R-project.org/package=caret}, ## } ## ## ATTENTION: This citation information has been auto-generated from ## the package DESCRIPTION file and may need manual editing, see ## &#39;help(&quot;citation&quot;)&#39;. citation(package = &quot;dplyr&quot;) ## ## To cite package &#39;dplyr&#39; in publications use: ## ## Hadley Wickham, Romain François, Lionel Henry and Kirill Müller ## (2019). dplyr: A Grammar of Data Manipulation. R package version ## 0.8.1. https://CRAN.R-project.org/package=dplyr ## ## A BibTeX entry for LaTeX users is ## ## @Manual{, ## title = {dplyr: A Grammar of Data Manipulation}, ## author = {Hadley Wickham and Romain François and Lionel Henry and Kirill Müller}, ## year = {2019}, ## note = {R package version 0.8.1}, ## url = {https://CRAN.R-project.org/package=dplyr}, ## } citation(package = &quot;ggplot2&quot;) ## ## To cite ggplot2 in publications, please use: ## ## H. Wickham. ggplot2: Elegant Graphics for Data Analysis. ## Springer-Verlag New York, 2016. ## ## A BibTeX entry for LaTeX users is ## ## @Book{, ## author = {Hadley Wickham}, ## title = {ggplot2: Elegant Graphics for Data Analysis}, ## publisher = {Springer-Verlag New York}, ## year = {2016}, ## isbn = {978-3-319-24277-4}, ## url = {https://ggplot2.tidyverse.org}, ## } "]
]
